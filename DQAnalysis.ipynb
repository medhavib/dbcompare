{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be4676-841d-4533-9472-de7812f50f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This utility can take multiple CSV output files, each with same format and produce the delta of changes between the \n",
    "#files. Each file is assumed to be a database comparison between two databases that are synced using some process or tool.\n",
    "#It also takes care of data sync lag between the data sources by removing records that may not have yet have been confirmed \n",
    "#to be different (could be different because of a time lag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dbc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default metrics dict\n",
    "#add fields here...\n",
    "metrics_base = {\n",
    "        'f1_mismatch':0,\n",
    "        'f2_mismatch':0\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aee8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup of the dataframe is the first step - here we baseline values\n",
    "default_values = {\n",
    "    'f1_src1': 'UNSET', 'f1_src2': 'UNSET', \n",
    "    'f2_src1': 'NOT_COMPLETED', 'f2_src2': 'NOT_COMPLETED'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dq_clean(dq_df):\n",
    "    \n",
    "    dq_df = dq_df.fillna(value=default_values)\n",
    "    \n",
    "    #add metrics columns inside the dataframe as well\n",
    "    for metric, value in metrics_base.items():\n",
    "        dq_df[metric] = value\n",
    "    \n",
    "    return dq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d71ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(row):\n",
    "    if row['f1_src1'] != row['f1_src2']:\n",
    "        row['mismatch'] = row['f1_mismatch'] = True\n",
    "    if row['f2_src1'] != row['f2_src2']:\n",
    "        row['mismatch'] = row['f2_mismatch'] = True        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_metrics(dq_df):\n",
    "    #initialize the metrics to 0 first\n",
    "    metrics = dict.fromkeys(metrics_base, 0)\n",
    "    metrics['f1_mismatch'] = dq_df.value_counts('f1_mismatch')[True] if True in dq_df.value_counts('f1_mismatch') else 0\n",
    "    metrics['f2_mismatch'] = dq_df.value_counts('f2_mismatch')[True] if True in dq_df.value_counts('f2_mismatch') else 0\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dq_analyze(dq_df, dq_date):\n",
    "    dq_df = dq_df.apply(update_metrics, axis=1)\n",
    "    \n",
    "    #drop the rows that dont have mismatch that we want to consider\n",
    "    dq_df = dq_df[dq_df['mismatch'] == True]\n",
    "    \n",
    "    return dq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dq_dfDict = OrderedDict()\n",
    "\n",
    "def dq_df_add(dq_df, dq_date):\n",
    "    \n",
    "    dfB = dq_df.set_index('main_key')\n",
    "    for date, dfA in dq_dfDict.items():\n",
    "        \n",
    "        #step 1 is to provide only the increments over the previously found users, so if\n",
    "        #B is the new set, we need to do a B-A1-A2-A3 etc.\n",
    "  \n",
    "        #dfA = dfA.set_index('x_user_id') -- not needed since all previous dataframes in dict are having this index\n",
    "        dfB = dfB.loc[dfB.index.difference(dfA.index), ]\n",
    "        \n",
    "        #second order of business is to now do a A1-(A1-B), A2-(A2-B), A3-(A3-B) and rewrite all previous dataframes \n",
    "        #to remove any records we thought were previously not in sync but because they no longer appeared in B, that\n",
    "        #means that they are in sync now, so we need to remove them from A\n",
    "        dfB1 = dq_df.set_index('main_key')\n",
    "        dfX = dfA.loc[dfA.index.difference(dfB1.index), ]\n",
    "        dfA = dfA.loc[dfA.index.difference(dfX.index), ]\n",
    "        #dfA.to_excel(writer, sheet_name=date) - writing will be done all at once\n",
    "        #instead we need to change the item in dict\n",
    "        dq_dfDict[date] = dfA\n",
    "\n",
    "    #At the end of the loop, now we can write B down\n",
    "    #this also handles the case of the first element insertion where we do not enter the loop at all\n",
    "    dq_dfDict[dq_date] = dfB\n",
    "    #dfB.to_excel(writer, sheet_name=dq_date) -- no need to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6af625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a special date based metrics dictionary\n",
    "metrics_all = {}\n",
    "\n",
    "def dq_df_writeall():\n",
    "    \n",
    "    writer = pd.ExcelWriter('dqanalysis.xlsx', mode = 'w')\n",
    "    \n",
    "    first = True\n",
    "    for date, df in dq_dfDict.items():\n",
    "        df.to_excel(writer, sheet_name=date)\n",
    "        metrics_all[date + ('' if first else '-new')] = sum_metrics(df)\n",
    "        first = False\n",
    "        \n",
    "    df = pd.DataFrame(metrics_all)\n",
    "    df.to_excel(writer, sheet_name=\"DQ Metrics\")\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each file that matches a filename pattern, we will load the file, print out the numbers\n",
    "#as listed based on column mismatches and then we will put the numbers in a comparison array with the date of file \n",
    "#as one of the parameters. Then we should be able to establish a trend.\n",
    "#example file name: participant_info_12_09_2023_01_03_40.xlsx\n",
    "\n",
    "import os,glob\n",
    "searchedfiles = sorted(glob.glob(\"./dbcompare_info_*.xlsx\"), key=os.path.getmtime)\n",
    "\n",
    "for fname in searchedfiles:\n",
    "    #date extraction\n",
    "    m = re.match(r'.\\/dbcompare_info_(\\d\\d)_(\\d\\d)_(\\d\\d\\d\\d).*', fname)\n",
    "    dq_date = m.group(2) + \"-\" + m.group(1) + \"-\" + m.group(3)\n",
    "    print(dq_date)\n",
    "    \n",
    "    dq_df = pd.read_excel(fname, 'differences')\n",
    "    \n",
    "    #metrics_base = dict.fromkeys(metrics_base, False)\n",
    "    dq_df = dq_clean(dq_df)\n",
    "    dq_df = dq_analyze(dq_df, dq_date)\n",
    "    dq_df_add(dq_df, dq_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_df_writeall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
